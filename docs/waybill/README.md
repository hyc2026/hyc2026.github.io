# 基于运单数据的地址知识图谱构建与应用

## 研究背景

地址文本数据的处理主要是为了提高地址数据的质量和准确性，其需求广泛存在于地图业务的各个方面，如地理位置服务、POI 数据的质量提升、POI 数据聚合等。但是，地址文本数据本身来自于不同的来源，并且遵循不同的范式，从而导致文本数据中混有大量的错误和信息缺失。这些问题给地址文本数据中有效信息的挖掘带来了极大的困难，难以满足地图业务对其进行深度分析和处理的巨大需求。因此，基于该场景独特的特征和需求，对现有的知识图谱技术进行拓展，针对性地对地址知识图谱的构建和应用技术进行系统深入的探索与研究，就显得愈发重要。

近年来，深度学习技术的发展极大地改变了人们的生活，但是在专业领域上仍然存在一定的局限性，主要包括以下三个方面：1）其对海量的有标签训练数据的依赖；2）通用领域模型难以迁移到垂直领域中；3）模型的结果可解释性差，难以直接应用在实际生产环境中。

知识图谱技术恰好是解决上述问题的有效工具。知识图谱以符号化的方式描述真实世界中的实体及其属性和相互关系，并将它们组织成事实三元组的图形结构。知识图谱作为显式模型，可以通过自动抽取结构化数据、使用语义约束进行推理等方式为深度学习模型提供垂直领域知识，有助于提高模型的准确性、灵活性和可解释性。此外，知识图谱还可以通过构建多种实体间关系来为深度学习模型提供更丰富的背景知识，从而提高模型的鲁棒性。同时，深度学习模型强大的学习推理能力可以反向为知识图谱的构建与扩展提供支撑。因此，知识图谱技术与深度学习技术两者互为补充，相互增强。

然而，知识图谱技术在遇到地址文本这一专业领域时，面临如下四个方面的挑战：1）地址文本数据规模大，且存在大量的内容错误和信息缺失；2）地址文本数据存在多样性和模糊性，地址文本包含歧义和冗余信息；3）地址实体间存在关系复杂的层级结构；4）地址文本数据具有领域内独特的特征，且下游业务也有其特殊的业务需求。这些挑战给地址知识图谱这一垂直领域图谱的构建与应用研究带来了巨大的困难。

## 研究内容与研究目标

研究内容：针对地址文本数据具有噪声多、层级关系复杂等特性，为充分挖掘和利用地址文本数据中的有效信息，并构建支撑地图业务这一垂直领域的地址知识图谱，本项目主要研究内容如下：
1. 研究基于运单数据的地址实体抽取技术，优化运单文本数据中可能存在的大量的噪音实体、长地址实体及嵌套地址实体的问题，实现精准且高效地从海量运单文本数据中抽取出预定义的关键地址实体信息，为地址知识图谱的精准构建提供基础支撑；
2. 研究基于运单数据的地址实体链接技术，优化运单文本数据中存在的“一址多名”问题，实现运单数据文本中已识别的实体对象，无歧义且正确映射到标准地址知识库中目标实体，有效缓解运单数据的歧义和冗余信息，为构建高可用性和可靠性的地址知识图谱提供有效支持；
3. 研究基于运单数据的地址知识图谱构建技术，结合地图业务中面临的实际应用需求和场景，研发面向地图领域智能化的地址知识图谱原型系统，将运单数据中涉及的地址信息构建成结构化的图形网络，高效辅助地图业务中一体化地址实体查询、路线规划、位置检索、地理数据分析等功能。

研究目标：本项目聚焦于面向运单数据的地址知识图谱构建技术，主要着眼于地址文本数据独特的特征和需求，力争在现有知识图谱技术的基础上适应性调整和扩展，构建一套智能化的地址知识图谱原型系统，支持地图业务中地址文本深度分析和处理的实际应用需求。

## 基于运单数据的地址实体抽取技术

**1、实体抽取技术研究背景**
命名实体识别 (Named Entity Recognition， NER) 是一项基本的信息提取任务，在信息检索、知识图谱等自然语言处理应用中起着至关重要的作用。 NER 的目标是从句子中提取一些预定义的特定实体，并识别出它们的正确类型，例如人物、位置、组织。近年来，随着研究的不断深入，深度学习已成为直接从数据中学习特征表示的强大策略，它无需复杂的特征工程和丰富的领域知识，即可学习复杂的隐藏表示，为自然语言处理领域带来了显著突破。基于深度学习的方法在NER中也已经压倒性地超越了传统的基于规则的方法和基于统计的方法，成为当前研究的主流方法。

一般基于深度学习的方法将NER定义为序列标注任务，即给定输入文本序列，要求模型判断序列中实体的开始位置、中间位置、结束位置等。相比于已经研究较多的通用英文语料下的实体识别，我们所要解决的基于运单文本数据的地址实体识别技术场景更加特殊，也更富有挑战性：

1. 中文词语边界模糊的特点为中文实体识别带来挑战。在英语中，单词之间有分隔符来标识边界，每个单词都有完整的含义。而在汉语中，一些汉字有其独立的意义，但更多的汉字需要与其他汉字组合才能形成一个有意义的词。汉字作为文本的基本单位，在组词时没有明确的词分隔符，模糊的词边界会造成很多边界歧义，增加中文命名实体边界界定的难度。
2. 中文实体组成结构的复杂性在地址场景下尤为明显。一个典型的场景就是人名和地名之间往往具有汉字上的重合，例如“山”字往往出现在人名或地名中，“中山”即可能是人名，也可能是地名，也可能是一座山的标识。而且，即使是同类型的实体，对应的汉字数量也可能长短不一，差异巨大。这种实体组成结构的复杂性增加了中文地址场景下实体抽取的难度。
3. 新实体的快速增加也是中文地址实体识别的一项挑战。运单地址中常包含店铺、商场等实体，该类实体经常更换名字，导致不断会有形态各异的新实体需要动态识别更新。这对模型的泛化性提出了更高的要求，也为运单实体识别带来的更大的挑战。

**2、模型算法设计**
为了解决上述基于中文运单数据的地址实体识别问题，得到一个高质量的地址实体识别算法，我们对近几年在顶级学术会议上新提出的技术进行了调研，复现了其中一些效果较好的算法，并在公开数据集和腾讯开放的地址实体标注数据集上进行了初步的实验。具体来说，我们尝试了以下几种方法：

1. 基于BERT预训练模型与FGM对抗学习的BERT-FGM-NER模型。这是一种简单高效的通用NER的模型，它以BERT或其他预训练模型作为文本的编码器，得到每个文字的语义向量，并基于每个文字的表示向量对其进行分类，判断是否是实体的开头、结尾等。为了进一步增强模型的鲁棒性和泛化性，我们在该模型的基础上加入FGM对抗训练，在文字表示向量上加入噪音扰动，并要求加上扰动后的输出分布与原本的输出分布相同。
2. 复旦大学于2020年在ACL2020上提出的FLAT模型。之前的工作虽然引入词汇信息来提升中文NER的效果，但在引入过程中存在信息损失和计算效率低的问题。
针对以上问题，FLAT模型的设计基于Transformer结构，提出了头位置向量和尾位置向量，对于每个char和word，分别使用头编号Head和尾编号Tail来编码其位置把四种相对距离融合进其设计的相对位置编码当中，FLAT模型结构如图1所示，其设计的主要优势如下：

1. 使用的位置编码方式集合外加的词汇边界信息可以提高模型对实体边界的识别效果；
2. 使用了词汇的词向量，可以提高模型对实体类别的识别能力；
3. 模型运算效率高。

## 基于运单数据的地址实体链接技术

实体链接的任务是将文本中出现的实体提及与知识库中的相应实体联系起来。实体提及(mention)是文本中的一个token序列，它可能指的是某个命名实体(entity)。Entity是一个由知识库明确定义的具有唯一标识符的词或短语，它可以是一个现实世界的对象（如组织和个人）或一个抽象的概念（如歌曲和事件）。大多数现有的实体链接工作都假设mention的边界已知，如使用命名实体识别（NER）工具等。

由于名字的变化和实体的模糊性，实体链接任务是具有挑战性的。一方面，一个被命名的实体可能有许多不同的表面形式（例如，全名、部分名称、昵称、别名和缩写）。另一方面，一个实体提及可以指代许多不同的命名实体。

基于传统机器学习的实体链接方法主要是利用人工设计的实体流行度等特征，其实体排名技术可以大致分为两类：无监督的和有监督的排名方法。无监督的排名方法包括基于向量空间模型的方法和基于信息检索的方法。有监督的排名方法主要包括二分类方法、学习排名方法、概率方法和基于图的方法等。

大多数传统的基于机器学习的实体链接方法都遵循流行的两步程序，首先提取一些人工筛选的特征，然后将这些特征反馈给实体排名方法，以做出最终的链接预测。然而，这些方法有两个局限性。(1)良好性能的特征需要大量仔细和繁琐的特征工程；(2)由于在设计特征的过程中对特定的知识库和领域知识的强烈依赖，将训练好的实体链接模型推广到其他知识库或领域是困难的。

如今，主流的方法通过使用深度学习模型进行实体和提及的特征提取及排名，与传统的机器学习相比，深度学习可以自动学习重要的特征，并且更容易从一个领域迁移到另一个领域。

一个典型的实体链接系统通常由三个阶段组成。
1.	候选实体生成阶段可以利用名称字典和网络信息为每个实体的提及生成一个候选映射实体集； 
2.	候选实体排名阶段可以利用不同种类的证据对候选实体进行排名；
3.	不可连接的提及预测阶段可以验证该实体的提及是否应该被标注为NIL。

一三阶段的算法较为固定且优化空间不大，因此实体链接任务的难点主要集中在第二阶段，针对运单数据地址的特点，我们使用了基于预训练模型的cross encoder模型。

### 公开数据集

**Flat NER**

![dataset](dataset.png)

### 公开baseline

[MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition](https://arxiv.org/abs/2107.05418)

[Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter](https://arxiv.org/abs/2105.07148)

[FLAT: Chinese NER Using Flat-Lattice Transformer](https://arxiv.org/abs/2004.11795)

[A Unified MRC Framework for Named Entity Recognition](https://arxiv.org/abs/1910.11476)

