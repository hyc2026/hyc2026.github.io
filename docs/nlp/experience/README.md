## NLP

### word2vec(CBOW&Skip-gram)和GloVe

word2vec哪种训练方式更好

word2vec是NNLM的一个较为典型的代表，其利用了DNN的方法来训练获取到词向量，而且词向量是中间产物，这个思路最早是Bengio提出，Google Brain提出的word2vec让词向量火了起来。而GloVe是采用词频统计信息，利用一些数学方法逐步优化得来，它没有神经网络的结构，所以词向量训练的速度相对更快。（这里当时不记得具体的公式推导了，原论文我倒是看过，但是当时记得不清了，实际上GloVe是词共现矩阵+类SVD的方法）

word2vec从宏观上描述了DNN的一个结构，从输入（大致带过分词，词表构建，one-hot等过程），到隐层，到输出层。然后详细讲了两种训练结构，即CBoW和Skip-Gram，但是当时这两种方法被我说反了。（当时并无觉察）讲完两种训练方法后，大致介绍了下训练时候词表大小过大，输出层过大的优化方法，即：**hierarchical softmax**和**negative sampling**。

#### Skip-Gram

[理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)

*下文中所有的Word2Vec都是指Skip-Gram模型*

Word2Vec模型实际上分为了两个部分，**第一部分为建立模型，第二部分是通过模型获取嵌入词向量。**Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。

![skip_gram_data](skip_gram_data.png)

![skip_gram](skip_gram.jpg)

**高效训练**

1. 将常见的单词组合（word pairs）或者词组作为单个“words”来处理。

2. 对高频次单词进行抽样来减少训练样本的个数。

   某个单词被留下的概率：$P(w_i)=(\sqrt\frac{Z(w_1)}{0.001}+1)\times\frac{0.001}{Z(w_i)}$，其中$Z(w_i)$是单词$w_i$出现的频率，0.001是一个超参数，文档中对这个参数的解释为“ threshold for configuring which higher-frequency words are randomly downsampled”。这个值越小意味着这个单词被保留下来的概率越小。

3. 对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。

   当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新。

   >  在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。

   一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。

#### GloVe

GloVe模型既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。

[详解GloVe词向量模型](https://blog.csdn.net/buchidanhuang/article/details/98471741)

[通俗易懂理解——Glove算法原理](https://zhuanlan.zhihu.com/p/42073620)

###                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Subword

#### Byte Pair Encoding

BPE(字节对)编码或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。 后期使用时需要一个替换表来重建原始数据。OpenAI GPT-2与Facebook RoBERTa均采用此方法构建subword vector。

- 优点: 可以有效地平衡词汇表大小和步数(编码句子所需的token数量)。
- 缺点: 基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。

**算法**

1. 准备足够大的训练语料
2. 确定期望的subword词表大小
3. 将单词拆分为字符序列并在末尾添加后缀"</ w>"，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为"low</ w>"：5
4. 统计每一个连续字节对的出现频率，选择最高频者合并成新的subword
5. 重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1

停止符"</ w>"的意义在于表示subword是词后缀。举例来说："st"字词不加"</ w>"可以出现在词首如"st ar"，加了"</ w>"表明改字词位于词尾，如"wide st</ w>"，二者意义截然不同。

每次合并后词表可能出现3种变化：

- +1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）
- +0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）
- -1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）

实际上，随着合并的次数增加，词表大小通常先增加后减小。

**编码和解码**

- 编码

在之前的算法中，我们已经得到了subword的词表，对该词表按照子词长度由大到小排序。编码时，对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一。

我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<unk>。

例子

```text
# 给定单词序列
[“the</w>”, “highest</w>”, “mountain</w>”]

# 假设已有排好序的subword词表
[“errrr</w>”, “tain</w>”, “moun”, “est</w>”, “high”, “the</w>”, “a</w>”]

# 迭代结果
"the</w>" -> ["the</w>"]
"highest</w>" -> ["high", "est</w>"]
"mountain</w>" -> ["moun", "tain</w>"]
```

编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。 如果我们看到字典中不存在的未知单词。 我们应用上述编码方法对单词进行tokenize，然后将新单词的tokenization添加到字典中备用。

- 解码

将所有的tokens拼在一起。

例子：

```python
# 编码序列
[“the</w>”, “high”, “est</w>”, “moun”, “tain</w>”]

# 解码序列
“the</w> highest</w> mountain</w>”
```

#### WordPiece

Google的Bert模型在分词的时候使用的是WordPiece算法。与BPE算法类似，WordPiece算法也是每次从词表中选出两个子词合并成新的子词。与BPE的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并，而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。

看到这里，你可能不清楚WordPiece是如何选取子词的。这里，通过形式化方法，能够清楚地理解WordPiece在合并这一步是如何作出选择的。假设句子$S=(t_1,t_2,...,t_n)$由n个子词组成，$t_i$表示子词，且假设各个子词之间是独立存在的，则句子$S$的语言模型似然值等价于所有子词概率的乘积：
$$
logP(S)=\sum_{i=1}^nlogP(t_i)
$$
假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子$S$似然值的变化可表示为：
$$
logP(t_z)-(logP(t_x)+logP(t_y))=log(\frac{P(t_z)}{p(t_x)p(t_y)})
$$
从上面的公式，很容易发现，似然值的变化就是两个子词之间的互信息。简而言之，WordPiece每次选择合并的两个子词，他们具有最大的互信息值，也就是两子词在语言模型上具有较强的关联性，它们经常在语料中以相邻方式同时出现。

#### Unigram Language Model

与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。ULM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。

我们接下来看看ULM是如何操作的。

对于句子S，$\vec{x}=(x_1,x_2,...x_m)$为句子的一个分词结果，由m个子词组成。所以，当前分词下句子S的似然值可以表示为：
$$
P(\vec{x})=\prod_{i=1}^mP(x_i)
$$
对于句子S，挑选似然值最大的作为分词结果，则可以表示为
$$
x^*=arg\ \underset{\it{x\in U(x)}}{\rm{max}}P(\vec{x})
$$
这里$U(x)$包含了句子的所有分词结果。在实际应用中，词表大小有上万个，直接罗列所有可能的分词组合不具有操作性。针对这个问题，可通过维特比算法得到$x^*$来解决。

那怎么求解每个子词的概率$P(x_i)$呢？ULM通过EM算法来估计。假设当前词表V, 则M步最大化的对象是如下似然函数：
$$
L=\sum_{s=1}^{|D|}log(P(X^{(s)}))=\sum_{s=1}^{|D|}log(\sum_{x\in U(X^{(s)})}P(x))
$$
其中，|D|是语料库中语料数量。上述公式的一个直观理解是，将语料库中所有句子的所有分词组合形成的概率相加。

但是，初始时，词表V并不存在。因而，ULM算法采用不断迭代的方法来构造词表以及求解分词概率：

1. 初始时，建立一个足够大的词表。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。
2. 针对当前词表，用EM算法求解每个子词在语料上的概率。
3. 对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss。
4. 将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。
5. 重复步骤2到4，直到词表大小减少到设定范围。

可以看出，ULM会保留那些以较高频率出现在很多句子的分词结果中的子词，因为这些子词如果被丢弃，其损失会很大。

### Transformer、ELMo、GPT和Bert

#### Attention

**3大优点**

1. 参数少: 模型复杂度跟CNN、RNN相比，复杂度更小，参数也更少。所以对算力的要求也就更小。
2. 速度快: Attention 解决了 RNN 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。
3. 效果好: 在 Attention 机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。Attention 是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息。下图红色的预期就是被挑出来的重点。

[Transformer](https://hyc2026.github.io/#/Transformer/Transformer)

[PTMs：NLP预训练模型的全面总结](https://zhuanlan.zhihu.com/p/115014536)

### HMM和CRF

当时HMM的具体理论在准备阶段就大致地看了下，面试官很nice，没有很为难，理论的不记得那来实践的，就接着这个问题，问了HMM的几个要素，即：初始概率，状态转移矩阵，发射矩阵，这三个要素，然后我主要讲了下这三个要素的运算过程，提及了一下维特比算法。（这里当时准备的不充分，说的不是特别清楚，后来我去恶补了一下）

维特比算法其实是一种动态规划算法，动态规划算法通常用来解决什么问题，在HMM里是怎么使用的？
大致描述了下动态规划的最优解问题，然后结合HMM的迭代过程说了一些。（后来仔细看了下，感觉面试官应该还是想听到HMM的理论，因为HMM推导会用到它里面的假设，然后得到递推关系，就可以分解为子问题，利用维特比算法求解）

### RNN、LSTM、GRU、BiLSTM

lstm为什么是sigmoid函数和tanh函数，可否换成别的激活函数

lstm的参数量

### 正则化

#### Dropout

Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一定比例的特征检测器（让其隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。

**具体过程：**

1. 首先随机（临时）删掉网络中一定比例的隐藏神经元，输入输出神经元保持不变

2. 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。

3. 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）

不断重复这一过程。

**作用：**

取平均的作用、减少神经元之间复杂的共适应关系

**实现**

dropout 有两种实现方式，**Vanilla Dropout** 和 **Inverted Dropout**

![dropout](2dropout.png)

```python
class Dropout(InplaceFunction):
    ... ...
    @classmethod
    def forward(cls, ctx, input, p=0.5, train=False, inplace=False):
        ctx.p = p
        ctx.train = train
        
        if ctx.p == 0 or not ctx.train: # infer阶段保持不变
            return input
        
        output = input.clone()
        
        ctx.noise = cls._make_noise(input) # mask
        if ctx.p == 1:
            ctx.noise.fill_(0)
        else:
            ctx.noise.bernoulli_(1 - ctx.p).div_(1 - ctx.p) # 对mask做1/(1-p)倍放大
        ctx.noise = ctx.noise.expand_as(input)
        output.mul_(ctx.noise)
        
        return output
    
    @classmethod
    def backward(ctx, grad_output):
        if ctx.p > 0 and ctx.train:
            return grad_output * ctx.noise, None, None, None # 梯度和输入做同样的mask和放大
        else:
            return grad_output, None, None, None
```

#### L1和L2正则化：

我们所说的正则化，就是在原来的loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项。

优化目标：
$$
min\ 1/N*\sum_{i=1}^N(y_i-w^Tx_i)^2
$$
L1正则
$$
min\ 1/N*\sum_{i=1}^N(y_i-w^Tx_i)^2 + C||w||_1
$$
L2正则
$$
min\ 1/N*\sum_{i=1}^N(y_i-w^Tx_i)^2 + C||w||_2^2
$$
结构风险最小化： 在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度

最终加入**L1**范数得到的解，一定是某个菱形和某条原函数等高线的切点。经过观察可以看到，几乎对于很多原函数等高曲线，和某个菱形相交的时候及其容易相交在坐标轴，也就是说最终的结果，解的某些维度及其容易是0，这也就是我们所说的L1更容易得到稀疏解（解向量中0比较多)的原因。

当加入**L2**正则化的时候，分析和L1正则化是类似的，也就是说我们仅仅是从菱形变成了圆形而已，同样还是求原曲线和圆形的切点作为最终解。当然与L1范数比，我们这样求的L2范数的，不容易交在坐标轴上，但是仍然比较靠近坐标轴**。**因此这也就是我们老说的，L2范数能让解比较小（靠近0），但是比较平滑（不等于0）。

**正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。**

**给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。**

**L1正则化和L2正则化：**

**L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。**

### 各种Loss

### 优化器SGD和Adam

[深度学习优化方法总结比较](https://zhuanlan.zhihu.com/p/22252270)

### 激活函数

## 语言模型

[综述：神经网络语言模型](https://zhuanlan.zhihu.com/p/109564205)

`FFNNLM`: FFNN的输入必须是固定长度，因此将前n-1个词作为预测下一个词的上下文。FFNNLM通过学习到每个词的分布式表示，来实现在连续空间上的建模。其中，词的表示只是LM用于提升其他NLP任务的副产品。CBOW和Skip-gram是基于FFNNLM提出的。

`RNNLM`

`LSTM-RNNLM`

**降低困惑度的技术**

字符感知模型

因子模型

双向模型

缓存机制

注意力机制

`ELMo`

`GPT`

`Bert`

## 简历准备

1. 整体内容一定要跟岗位需求match，有的放矢。match的内容展开写，不match的一笔带过或者直接不写
2. 把最有信心在面试中谈起的经历所在的板块写在最前面（仅次于教育经历），并用配色突出这条经历

## 面试中的基础知识

> 方向不match的面试官喜欢考察**词向量和文本分类**相关的知识

**模型篇**

- SGNS/cBoW、FastText、ELMo等（从词向量引出）
- DSSM、DecAtt、ESIM等（从问答&匹配引出）
- HAN、DPCNN等（从分类引出）
- BiDAF、DrQA、QANet等（从MRC引出）
- CoVe、InferSent等（从迁移引出）
- MM、N-shortest等（从分词引出）
- Bi-LSTM-CRF等（从NER引出）
- LDA等主题模型（从文本表示引出）

**训练篇**

- point-wise、pair-wise和list-wise（匹配、ranking模型）
- 负采样、NCE
- 层级softmax方法，哈夫曼树的构建
- 不均衡问题的处理
- KL散度与交叉熵loss函数

**评价指标篇**

- F1-score
- PPL
- MRR、MAP

## TIPS

1. 一定不要错过提前批！一定不要错过提前批！一定不要错过提前批！不要相信错过提前批还有正式批的鬼话，很多核心部门的NLP岗的hc在提前批就用光了！
2. 提前批不要拖到末尾！尤其百度自然语言处理部的坑，面试当天就给口头offer，先占先得！
3. 阿里基本是远程面试，6轮面试都没编程题！简历内容准备好就可以直接投阿里了！
4. 内推之前千万不要把简历挂到腾讯校招系统，填上意向部门都没用，不match的其他部门很可能以迅雷不及掩耳之势强行捞起你的简历，然后你的腾讯之旅就举步维艰了（我跟另外俩小伙伴血的教训。。。）
5. 如果只是想找算法岗，但没有研究方向，没刷几篇paper，不过研发能力强，基础编程题解的快，工具用的熟，那就去投头条吧。（亲测它提前批的三轮技术面都没学术味儿，所以优势在对立面的小伙伴谨慎）